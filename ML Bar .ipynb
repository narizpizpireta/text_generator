{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cs-training.csv\")\n",
    "\n",
    "df = df.drop('Unnamed: 0', 1)\n",
    "\n",
    "df = df.dropna(axis=0, how = \"any\" )\n",
    "\n",
    "data = df.as_matrix()\n",
    "\n",
    "\n",
    "labels = []\n",
    "for i in range(data.shape[0]):\n",
    "    if data[i, 0] == 1.0:\n",
    "        labels.append([1.0, 0.0])\n",
    "    else:\n",
    "        labels.append([0.0, 1.0])\n",
    "\n",
    "feature = data[:, 1:]\n",
    "label = np.vstack(np.array(i, dtype = np.float32) for i in labels)\n",
    "\n",
    "x = feature \n",
    "y = label\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.70, random_state=1996)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.add(Dense(units=64, activation='relu', input_dim=10))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(x_train, y_train, epochs=50, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"wonderland_short.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  22471\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "#Summarize the dataset\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  22371\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize: scale to the range 0-to-1to make patterns easier to learn by the LSTM \n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Definethe LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 3.0747\n",
      "Epoch 00001: loss improved from inf to 3.07402, saving model to weights-improvement-01-3.0740.hdf5\n",
      "22371/22371 [==============================] - 139s 6ms/step - loss: 3.0740\n",
      "Epoch 2/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 3.0206\n",
      "Epoch 00002: loss improved from 3.07402 to 3.02018, saving model to weights-improvement-02-3.0202.hdf5\n",
      "22371/22371 [==============================] - 135s 6ms/step - loss: 3.0202\n",
      "Epoch 3/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 3.0076\n",
      "Epoch 00003: loss improved from 3.02018 to 3.00756, saving model to weights-improvement-03-3.0076.hdf5\n",
      "22371/22371 [==============================] - 144s 6ms/step - loss: 3.0076\n",
      "Epoch 4/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.9563\n",
      "Epoch 00004: loss improved from 3.00756 to 2.95633, saving model to weights-improvement-04-2.9563.hdf5\n",
      "22371/22371 [==============================] - 148s 7ms/step - loss: 2.9563\n",
      "Epoch 5/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.8848\n",
      "Epoch 00005: loss improved from 2.95633 to 2.88437, saving model to weights-improvement-05-2.8844.hdf5\n",
      "22371/22371 [==============================] - 135s 6ms/step - loss: 2.8844\n",
      "Epoch 6/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.8438\n",
      "Epoch 00006: loss improved from 2.88437 to 2.84418, saving model to weights-improvement-06-2.8442.hdf5\n",
      "22371/22371 [==============================] - 141s 6ms/step - loss: 2.8442\n",
      "Epoch 7/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.8082\n",
      "Epoch 00007: loss improved from 2.84418 to 2.80817, saving model to weights-improvement-07-2.8082.hdf5\n",
      "22371/22371 [==============================] - 136s 6ms/step - loss: 2.8082\n",
      "Epoch 8/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.7868\n",
      "Epoch 00008: loss improved from 2.80817 to 2.78594, saving model to weights-improvement-08-2.7859.hdf5\n",
      "22371/22371 [==============================] - 136s 6ms/step - loss: 2.7859\n",
      "Epoch 9/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.7648\n",
      "Epoch 00009: loss improved from 2.78594 to 2.76536, saving model to weights-improvement-09-2.7654.hdf5\n",
      "22371/22371 [==============================] - 136s 6ms/step - loss: 2.7654\n",
      "Epoch 10/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.7474\n",
      "Epoch 00010: loss improved from 2.76536 to 2.74735, saving model to weights-improvement-10-2.7474.hdf5\n",
      "22371/22371 [==============================] - 144s 6ms/step - loss: 2.7474\n",
      "Epoch 11/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.7319\n",
      "Epoch 00011: loss improved from 2.74735 to 2.73161, saving model to weights-improvement-11-2.7316.hdf5\n",
      "22371/22371 [==============================] - 148s 7ms/step - loss: 2.7316\n",
      "Epoch 12/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.7170\n",
      "Epoch 00012: loss improved from 2.73161 to 2.71741, saving model to weights-improvement-12-2.7174.hdf5\n",
      "22371/22371 [==============================] - 134s 6ms/step - loss: 2.7174\n",
      "Epoch 13/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.7014\n",
      "Epoch 00013: loss improved from 2.71741 to 2.70146, saving model to weights-improvement-13-2.7015.hdf5\n",
      "22371/22371 [==============================] - 134s 6ms/step - loss: 2.7015\n",
      "Epoch 14/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.6879\n",
      "Epoch 00014: loss improved from 2.70146 to 2.68774, saving model to weights-improvement-14-2.6877.hdf5\n",
      "22371/22371 [==============================] - 136s 6ms/step - loss: 2.6877\n",
      "Epoch 15/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.6687\n",
      "Epoch 00015: loss improved from 2.68774 to 2.66926, saving model to weights-improvement-15-2.6693.hdf5\n",
      "22371/22371 [==============================] - 137s 6ms/step - loss: 2.6693\n",
      "Epoch 16/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.6522\n",
      "Epoch 00016: loss improved from 2.66926 to 2.65328, saving model to weights-improvement-16-2.6533.hdf5\n",
      "22371/22371 [==============================] - 135s 6ms/step - loss: 2.6533\n",
      "Epoch 17/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.6350\n",
      "Epoch 00017: loss improved from 2.65328 to 2.63429, saving model to weights-improvement-17-2.6343.hdf5\n",
      "22371/22371 [==============================] - 136s 6ms/step - loss: 2.6343\n",
      "Epoch 18/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.6149\n",
      "Epoch 00018: loss improved from 2.63429 to 2.61448, saving model to weights-improvement-18-2.6145.hdf5\n",
      "22371/22371 [==============================] - 166s 7ms/step - loss: 2.6145\n",
      "Epoch 19/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.5949\n",
      "Epoch 00019: loss improved from 2.61448 to 2.59487, saving model to weights-improvement-19-2.5949.hdf5\n",
      "22371/22371 [==============================] - 149s 7ms/step - loss: 2.5949\n",
      "Epoch 20/20\n",
      "22272/22371 [============================>.] - ETA: 0s - loss: 2.5683\n",
      "Epoch 00020: loss improved from 2.59487 to 2.56803, saving model to weights-improvement-20-2.5680.hdf5\n",
      "22371/22371 [==============================] - 148s 7ms/step - loss: 2.5680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff74e548d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "\n",
    "filename = \"weights-improvement-20-2.5680.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" atch\n",
      "out of its waistcoat-pocket, and looked at it, and then hurried on,\n",
      "alice started to her feet,  \"\n",
      "and she was to the wabt the was to the wabt the was to the wabt the was to the wabt the was to the waat the was to the waat the was to the waat the was to the taat the was to the waat the was to the waat the was to the tat io the waot aa ine too to the wao to the wabt the was to the wabt the was to the wabt the was to the waat the was to the waat the was to the waat the was to the taat the was to the waat the was to the waat the was to the tat io the waot aa ine too to the wao to the wabt the was to the wabt the was to the wabt the was to the waat the was to the waat the was to the waat the was to the taat the was to the waat the was to the waat the was to the tat io the waot aa ine too to the wao to the wabt the was to the wabt the was to the wabt the was to the waat the was to the waat the was to the waat the was to the taat the was to the waat the was to the waat the was to the tat io the waot aa ine too to the wao to the wabt the was to the wabt the was to the wabt the was to the w\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#pick a random seed \n",
    "import sys \n",
    "\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
